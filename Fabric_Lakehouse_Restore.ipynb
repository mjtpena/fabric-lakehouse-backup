{"cells":[{"cell_type":"code","source":["# Microsoft Fabric Lakehouse Complete Restore Notebook\n","# Restores Tables and Files from unified backup ZIP\n","\n","# ============================================================================\n","# CELL 1: Restore Configuration Parameters (Parameterized Cell)\n","# ============================================================================\n","\n","# Parameters - Configure these values or override when scheduling\n","backup_source_type = \"lakehouse\"  # Options: \"storage_account\", \"lakehouse\", \"adls\", \"local_file\"\n","backup_storage_account = \"\"  # Required for storage_account type\n","backup_container = \"lakehouse-backups\"  # Container name for storage account\n","backup_lakehouse_name = \"lh_msft_bae_backup\"  # Required for lakehouse type\n","backup_workspace_id = \"WS-MSFT-BAE-DEMO-SBX\"  # REQUIRED: Backup workspace ID\n","backup_adls_account = \"\"  # Required for adls type\n","backup_adls_container = \"\"  # Required for adls type\n","backup_path = \"complete_backup_2025-07-03_20-41-04_64af2286\"  # REQUIRED: Path to the backup (auto-detect if empty)\n","\n","# Restore target configuration\n","target_lakehouse_name = \"lh_msft_bae_restore\"  # REQUIRED: Target lakehouse name\n","target_workspace_id = \"WS-MSFT-BAE-DEMO-SBX\"  # REQUIRED: Target workspace ID\n","\n","# Restore options\n","restore_tables = True  # Restore tables to Tables directory\n","restore_files = True   # Restore files to Files directory\n","table_format_preference = \"parquet\"  # Options: \"parquet\", \"csv\", \"auto\"\n","overwrite_existing = False  # Overwrite existing tables/files\n","create_target_lakehouse = False  # Create target lakehouse if it doesn't exist\n","restore_method = \"auto\"  # Options: \"auto\", \"zip_direct\", \"zip_from_delta\"\n","\n","# Advanced options\n","verify_restore = True  # Verify restored data\n","enable_detailed_logging = True  # Enable detailed logging\n","dry_run = False  # Show what would be restored without actually restoring\n","restore_specific_tables = []  # List of specific table names to restore (empty = all)\n","restore_specific_files = []  # List of specific file patterns to restore (empty = all)\n","use_managed_identity = True  # Use managed identity for external storage\n","\n","# ============================================================================\n","# CELL 2: Import Required Libraries and Initialize\n","# ============================================================================\n","\n","import os\n","import json\n","import datetime\n","import uuid\n","import zipfile\n","from io import BytesIO\n","from pyspark.sql.functions import lit, col, current_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, LongType, BooleanType, BinaryType\n","import time\n","import tempfile\n","\n","# Initialize global variables\n","log_entries = []\n","restore_start_time = datetime.datetime.now()\n","\n","# Check for Fabric utilities availability\n","fabric_utils_available = False\n","try:\n","    if 'mssparkutils' in dir():\n","        fabric_utils = mssparkutils\n","        fabric_utils_available = True\n","        utils_name = \"mssparkutils\"\n","    elif 'notebookutils' in dir():\n","        fabric_utils = notebookutils\n","        fabric_utils_available = True\n","        utils_name = \"notebookutils\"\n","    else:\n","        try:\n","            import notebookutils as fabric_utils\n","            fabric_utils_available = True\n","            utils_name = \"notebookutils\"\n","        except:\n","            try:\n","                import mssparkutils as fabric_utils\n","                fabric_utils_available = True\n","                utils_name = \"mssparkutils\"\n","            except:\n","                fabric_utils_available = False\n","                utils_name = \"none\"\n","except:\n","    fabric_utils_available = False\n","    utils_name = \"none\"\n","\n","print(\"📚 Libraries imported successfully\")\n","print(f\"🔧 Fabric utilities available: {fabric_utils_available} ({utils_name})\")\n","print(f\"🔄 Lakehouse restore process initiated at: {restore_start_time}\")\n","\n","# ============================================================================\n","# CELL 3: Core Helper Functions\n","# ============================================================================\n","\n","def get_current_timestamp():\n","    \"\"\"Return current timestamp in a standardized format\"\"\"\n","    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","def log_message(message, level=\"INFO\"):\n","    \"\"\"Log a message with timestamp and level\"\"\"\n","    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    print(f\"[{timestamp}] [{level}] {message}\")\n","    \n","    if enable_detailed_logging and level != \"DEBUG\":\n","        log_entries.append({\n","            \"timestamp\": timestamp,\n","            \"level\": level,\n","            \"message\": message\n","        })\n","\n","def validate_parameters():\n","    \"\"\"Validate that required parameters are provided\"\"\"\n","    if not target_lakehouse_name:\n","        raise ValueError(\"Target Lakehouse Name is required\")\n","    \n","    if not backup_path:\n","        raise ValueError(\"Backup path is required\")\n","    \n","    if backup_source_type == \"storage_account\" and not backup_storage_account:\n","        raise ValueError(\"Backup Storage Account is required for storage_account backup type\")\n","    elif backup_source_type == \"lakehouse\" and not backup_lakehouse_name:\n","        raise ValueError(\"Backup Lakehouse Name is required for lakehouse backup type\")\n","    elif backup_source_type == \"adls\" and (not backup_adls_account or not backup_adls_container):\n","        raise ValueError(\"Backup ADLS Account and Container are required for adls backup type\")\n","\n","def get_workspace_id():\n","    \"\"\"Get the current workspace ID\"\"\"\n","    try:\n","        methods = [\n","            (\"spark.fabric.workspaceId\", lambda: spark.conf.get(\"spark.fabric.workspaceId\", None)),\n","            (\"spark.sql.hive.metastore.warehouse.dir\", lambda: extract_workspace_from_warehouse_dir()),\n","            (\"WORKSPACE_ID env var\", lambda: os.environ.get(\"WORKSPACE_ID\", None)),\n","        ]\n","        \n","        for method_name, method_func in methods:\n","            try:\n","                workspace_id = method_func()\n","                if workspace_id:\n","                    log_message(f\"Found workspace ID using {method_name}: {workspace_id}\", \"DEBUG\")\n","                    return workspace_id\n","            except Exception as e:\n","                log_message(f\"Method {method_name} failed: {str(e)}\", \"DEBUG\")\n","        \n","        return None\n","        \n","    except Exception as e:\n","        log_message(f\"Error getting workspace ID: {str(e)}\", \"WARNING\")\n","        return None\n","\n","def extract_workspace_from_warehouse_dir():\n","    \"\"\"Extract workspace ID from warehouse directory path\"\"\"\n","    try:\n","        warehouse_dir = spark.conf.get(\"spark.sql.hive.metastore.warehouse.dir\", \"\")\n","        if \"onelake.dfs.fabric.microsoft.com\" in warehouse_dir:\n","            import re\n","            match = re.search(r'abfss://([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})@onelake', warehouse_dir)\n","            if match:\n","                return match.group(1)\n","        return None\n","    except Exception:\n","        return None\n","\n","def setup_external_storage_auth():\n","    \"\"\"Setup authentication for external storage if needed\"\"\"\n","    if backup_source_type in [\"storage_account\", \"adls\"] and use_managed_identity:\n","        log_message(\"Configuring managed identity for external storage authentication\", \"INFO\")\n","        \n","        try:\n","            if backup_source_type == \"storage_account\":\n","                account_name = backup_storage_account\n","                spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n","                spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n","            elif backup_source_type == \"adls\":\n","                account_name = backup_adls_account\n","                spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n","                spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n","            \n","            log_message(\"External storage authentication configured\", \"INFO\")\n","        except Exception as e:\n","            log_message(f\"Warning: Could not configure external storage auth: {str(e)}\", \"WARNING\")\n","\n","print(\"✅ Core helper functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 4: Path Construction Functions\n","# ============================================================================\n","\n","def get_backup_source_path():\n","    \"\"\"Construct path to the backup source based on the backup type\"\"\"\n","    if backup_source_type == \"storage_account\":\n","        return f\"abfss://{backup_container}@{backup_storage_account}.dfs.core.windows.net/{backup_path}\"\n","    \n","    elif backup_source_type == \"lakehouse\":\n","        if backup_workspace_id and backup_workspace_id.strip():\n","            workspace_id = backup_workspace_id\n","        else:\n","            workspace_id = get_workspace_id()\n","        \n","        if workspace_id:\n","            return f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{backup_lakehouse_name}.Lakehouse/Files/{backup_path}\"\n","        else:\n","            raise ValueError(\"Workspace ID is required for OneLake access. Please provide backup_workspace_id parameter.\")\n","    \n","    elif backup_source_type == \"adls\":\n","        return f\"abfss://{backup_adls_container}@{backup_adls_account}.dfs.core.windows.net/{backup_path}\"\n","    \n","    elif backup_source_type == \"local_file\":\n","        return backup_path\n","    \n","    else:\n","        raise ValueError(f\"Invalid backup source type: {backup_source_type}\")\n","\n","def get_target_paths():\n","    \"\"\"Construct paths to the target lakehouse Tables and Files directories\"\"\"\n","    if target_workspace_id and target_workspace_id.strip():\n","        workspace_id = target_workspace_id\n","    else:\n","        workspace_id = get_workspace_id()\n","    \n","    if workspace_id:\n","        base_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{target_lakehouse_name}.Lakehouse\"\n","        return {\n","            \"tables\": f\"{base_path}/Tables\",\n","            \"files\": f\"{base_path}/Files\"\n","        }\n","    else:\n","        raise ValueError(\"Workspace ID is required for OneLake access. Please provide target_workspace_id parameter.\")\n","\n","print(\"✅ Path construction functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 5: Backup Analysis and ZIP Reading Functions\n","# ============================================================================\n","\n","def analyze_backup_source(backup_source_path):\n","    \"\"\"Analyze the backup source and determine the restore method\"\"\"\n","    try:\n","        log_message(\"🔍 Analyzing backup source...\", \"INFO\")\n","        \n","        backup_info = {\n","            \"backup_type\": \"unknown\",\n","            \"has_zip_file\": False,\n","            \"has_zip_data\": False,\n","            \"has_manifest\": False,\n","            \"restore_method\": \"unknown\"\n","        }\n","        \n","        # Check for direct ZIP file\n","        try:\n","            if fabric_utils_available:\n","                files = fabric_utils.fs.ls(backup_source_path)\n","                for file in files:\n","                    if file.name.endswith('.zip'):\n","                        backup_info[\"has_zip_file\"] = True\n","                        backup_info[\"zip_file_path\"] = file.path\n","                        log_message(f\"✅ Found ZIP file: {file.name}\", \"INFO\")\n","                        break\n","        except Exception as e:\n","            log_message(f\"Could not list backup directory: {str(e)}\", \"WARNING\")\n","        \n","        # Check for ZIP data in Delta table\n","        try:\n","            zip_data_paths = [\n","                f\"{backup_source_path}/complete_backup_zip_data\",\n","                f\"{backup_source_path}/files_backup_zip_data\",\n","                f\"{backup_source_path}/lakehouse_complete_backup_zip_data\"\n","            ]\n","            \n","            for zip_path in zip_data_paths:\n","                try:\n","                    test_df = spark.read.format(\"delta\").load(zip_path)\n","                    test_df.limit(1).collect()\n","                    backup_info[\"has_zip_data\"] = True\n","                    backup_info[\"zip_data_path\"] = zip_path\n","                    log_message(f\"✅ Found ZIP data in Delta table: {zip_path}\", \"INFO\")\n","                    break\n","                except:\n","                    continue\n","                    \n","        except Exception as e:\n","            log_message(f\"Could not check for ZIP data: {str(e)}\", \"DEBUG\")\n","        \n","        # Check for manifest\n","        try:\n","            manifest_df = spark.read.format(\"delta\").load(f\"{backup_source_path}/_manifest\")\n","            manifest_data = manifest_df.collect()[0]\n","            backup_info[\"has_manifest\"] = True\n","            backup_info[\"manifest_data\"] = manifest_data.asDict()\n","            log_message(\"✅ Found backup manifest\", \"INFO\")\n","        except Exception as e:\n","            log_message(f\"No manifest found: {str(e)}\", \"DEBUG\")\n","        \n","        # Determine restore method\n","        if backup_info[\"has_zip_file\"]:\n","            backup_info[\"restore_method\"] = \"zip_direct\"\n","            backup_info[\"backup_type\"] = \"zip_file\"\n","        elif backup_info[\"has_zip_data\"]:\n","            backup_info[\"restore_method\"] = \"zip_from_delta\"\n","            backup_info[\"backup_type\"] = \"zip_in_delta\"\n","        else:\n","            backup_info[\"restore_method\"] = \"legacy_format\"\n","            backup_info[\"backup_type\"] = \"legacy\"\n","        \n","        log_message(f\"📋 Backup analysis complete: {backup_info['backup_type']} ({backup_info['restore_method']})\", \"INFO\")\n","        return backup_info\n","        \n","    except Exception as e:\n","        log_message(f\"Backup analysis failed: {str(e)}\", \"ERROR\")\n","        return {\"backup_type\": \"unknown\", \"restore_method\": \"unknown\", \"error\": str(e)}\n","\n","def read_zip_contents(backup_info):\n","    \"\"\"Read and analyze ZIP contents\"\"\"\n","    try:\n","        log_message(\"📦 Reading ZIP backup contents...\", \"INFO\")\n","        \n","        # Get ZIP binary data\n","        if backup_info[\"restore_method\"] == \"zip_direct\":\n","            # Read ZIP file directly\n","            if fabric_utils_available:\n","                zip_file_path = backup_info[\"zip_file_path\"]\n","                # Copy to local temp for processing\n","                temp_zip_path = f\"/tmp/restore_backup_{uuid.uuid4().hex[:8]}.zip\"\n","                fabric_utils.fs.cp(zip_file_path, f\"file:{temp_zip_path}\")\n","                \n","                with open(temp_zip_path, \"rb\") as f:\n","                    zip_binary = f.read()\n","                \n","                os.remove(temp_zip_path)\n","            else:\n","                raise Exception(\"Cannot read ZIP file without Fabric utilities\")\n","                \n","        elif backup_info[\"restore_method\"] == \"zip_from_delta\":\n","            # Read ZIP from Delta table\n","            zip_data_path = backup_info[\"zip_data_path\"]\n","            zip_df = spark.read.format(\"delta\").load(zip_data_path)\n","            zip_row = zip_df.collect()[0]\n","            zip_binary = zip_row.zip_binary\n","            \n","        else:\n","            raise Exception(f\"Cannot read ZIP for restore method: {backup_info['restore_method']}\")\n","        \n","        # Analyze ZIP contents\n","        zip_contents = {\n","            \"tables\": {},\n","            \"files\": [],\n","            \"metadata\": {},\n","            \"total_size\": len(zip_binary)\n","        }\n","        \n","        with zipfile.ZipFile(BytesIO(zip_binary), 'r') as zip_file:\n","            file_list = zip_file.namelist()\n","            \n","            log_message(f\"📁 ZIP contains {len(file_list)} items\", \"INFO\")\n","            \n","            # Parse contents\n","            for file_path in file_list:\n","                if file_path.startswith('tables/'):\n","                    # Parse table structure\n","                    path_parts = file_path.split('/')\n","                    if len(path_parts) >= 3:\n","                        table_name = path_parts[1]\n","                        file_name = path_parts[2]\n","                        \n","                        if table_name not in zip_contents[\"tables\"]:\n","                            zip_contents[\"tables\"][table_name] = {\n","                                \"formats\": [],\n","                                \"has_csv\": False,\n","                                \"has_parquet\": False,\n","                                \"has_schema\": False,\n","                                \"has_metadata\": False\n","                            }\n","                        \n","                        if file_name.endswith('.csv'):\n","                            zip_contents[\"tables\"][table_name][\"has_csv\"] = True\n","                            zip_contents[\"tables\"][table_name][\"formats\"].append(\"csv\")\n","                        elif file_name.endswith('.parquet'):\n","                            zip_contents[\"tables\"][table_name][\"has_parquet\"] = True\n","                            zip_contents[\"tables\"][table_name][\"formats\"].append(\"parquet\")\n","                        elif file_name == 'schema.json':\n","                            zip_contents[\"tables\"][table_name][\"has_schema\"] = True\n","                        elif file_name == 'metadata.json':\n","                            zip_contents[\"tables\"][table_name][\"has_metadata\"] = True\n","                \n","                elif file_path.startswith('files/'):\n","                    # Parse file structure\n","                    relative_path = file_path[6:]  # Remove 'files/' prefix\n","                    if relative_path and not relative_path.endswith('/'):\n","                        zip_contents[\"files\"].append({\n","                            \"zip_path\": file_path,\n","                            \"relative_path\": relative_path,\n","                            \"name\": relative_path.split('/')[-1],\n","                            \"type\": get_file_type(relative_path.split('/')[-1])\n","                        })\n","                \n","                elif file_path.startswith('_backup_info/'):\n","                    # Read metadata\n","                    if file_path.endswith('metadata.json'):\n","                        metadata_content = zip_file.read(file_path)\n","                        zip_contents[\"metadata\"] = json.loads(metadata_content.decode('utf-8'))\n","        \n","        log_message(f\"📊 Found {len(zip_contents['tables'])} tables and {len(zip_contents['files'])} files in ZIP\", \"INFO\")\n","        \n","        # Store ZIP binary for later use\n","        zip_contents[\"zip_binary\"] = zip_binary\n","        \n","        return zip_contents\n","        \n","    except Exception as e:\n","        log_message(f\"Failed to read ZIP contents: {str(e)}\", \"ERROR\")\n","        return None\n","\n","def get_file_type(filename):\n","    \"\"\"Determine file type based on extension\"\"\"\n","    if '.' not in filename:\n","        return \"unknown\"\n","    \n","    extension = filename.split('.')[-1].lower()\n","    \n","    file_types = {\n","        'jpg': 'image', 'jpeg': 'image', 'png': 'image', 'gif': 'image', 'bmp': 'image', 'svg': 'image', 'webp': 'image',\n","        'pdf': 'document', 'doc': 'document', 'docx': 'document', 'txt': 'text', 'rtf': 'document',\n","        'xls': 'spreadsheet', 'xlsx': 'spreadsheet', 'csv': 'data',\n","        'json': 'data', 'xml': 'data', 'yaml': 'data', 'yml': 'data', 'parquet': 'data',\n","        'zip': 'archive', 'rar': 'archive', '7z': 'archive', 'tar': 'archive', 'gz': 'archive',\n","        'py': 'code', 'sql': 'code', 'js': 'code', 'html': 'code', 'css': 'code',\n","        'mp4': 'video', 'avi': 'video', 'mov': 'video', 'mp3': 'audio', 'wav': 'audio'\n","    }\n","    \n","    return file_types.get(extension, 'other')\n","\n","print(\"✅ Backup analysis and ZIP reading functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 6: Fixed Table Restore Functions\n","# ============================================================================\n","\n","def restore_tables_from_zip(zip_contents, target_tables_path):\n","    \"\"\"Restore tables from ZIP backup - FIXED VERSION\"\"\"\n","    try:\n","        if not restore_tables:\n","            log_message(\"⏭️  Table restore skipped (restore_tables=False)\", \"INFO\")\n","            return {\"skipped\": True, \"reason\": \"restore_tables=False\"}\n","        \n","        tables_to_restore = zip_contents[\"tables\"]\n","        \n","        # Filter specific tables if requested\n","        if restore_specific_tables:\n","            tables_to_restore = {name: info for name, info in tables_to_restore.items() \n","                               if name in restore_specific_tables}\n","            log_message(f\"🎯 Restoring specific tables: {list(tables_to_restore.keys())}\", \"INFO\")\n","        \n","        if not tables_to_restore:\n","            return {\"success\": True, \"tables_restored\": 0, \"message\": \"No tables to restore\"}\n","        \n","        log_message(f\"📊 Starting table restore for {len(tables_to_restore)} tables...\", \"INFO\")\n","        \n","        zip_binary = zip_contents[\"zip_binary\"]\n","        tables_restored = 0\n","        tables_failed = 0\n","        restore_details = {}\n","        \n","        with zipfile.ZipFile(BytesIO(zip_binary), 'r') as zip_file:\n","            \n","            for table_name, table_info in tables_to_restore.items():\n","                try:\n","                    log_message(f\"  📋 Restoring table: {table_name}\", \"INFO\")\n","                    \n","                    if dry_run:\n","                        log_message(f\"    🔍 DRY RUN: Would restore table {table_name}\", \"INFO\")\n","                        restore_details[table_name] = {\"status\": \"dry_run\", \"formats\": table_info[\"formats\"]}\n","                        continue\n","                    \n","                    # Check if target table exists\n","                    target_table_path = f\"{target_tables_path}/{table_name}\"\n","                    table_exists = False\n","                    \n","                    try:\n","                        test_df = spark.read.format(\"delta\").load(target_table_path)\n","                        test_df.limit(1).collect()\n","                        table_exists = True\n","                    except:\n","                        table_exists = False\n","                    \n","                    if table_exists and not overwrite_existing:\n","                        log_message(f\"    ⚠️  Table {table_name} exists, skipping (overwrite_existing=False)\", \"WARNING\")\n","                        restore_details[table_name] = {\"status\": \"skipped\", \"reason\": \"exists\"}\n","                        continue\n","                    \n","                    # Determine which format to use for restore\n","                    restore_format = determine_table_restore_format(table_info, table_format_preference)\n","                    \n","                    df = None\n","                    \n","                    if restore_format == \"parquet\" and table_info[\"has_parquet\"]:\n","                        # Restore from Parquet - FIXED VERSION\n","                        parquet_data = zip_file.read(f\"tables/{table_name}/{table_name}.parquet\")\n","                        \n","                        # Use BytesIO instead of temp file\n","                        parquet_buffer = BytesIO(parquet_data)\n","                        \n","                        # Convert to pandas first, then to Spark\n","                        import pandas as pd\n","                        pandas_df = pd.read_parquet(parquet_buffer)\n","                        df = spark.createDataFrame(pandas_df)\n","                        \n","                        log_message(f\"    ✅ Read from Parquet: {table_name} ({len(pandas_df):,} rows)\", \"INFO\")\n","                        \n","                    elif restore_format == \"csv\" and table_info[\"has_csv\"]:\n","                        # Restore from CSV - FIXED VERSION\n","                        csv_data = zip_file.read(f\"tables/{table_name}/{table_name}.csv\")\n","                        \n","                        # Use BytesIO instead of temp file\n","                        from io import StringIO\n","                        csv_string = csv_data.decode('utf-8')\n","                        csv_buffer = StringIO(csv_string)\n","                        \n","                        # Convert to pandas first, then to Spark\n","                        import pandas as pd\n","                        pandas_df = pd.read_csv(csv_buffer)\n","                        df = spark.createDataFrame(pandas_df)\n","                        \n","                        log_message(f\"    ✅ Read from CSV: {table_name} ({len(pandas_df):,} rows)\", \"INFO\")\n","                        \n","                    else:\n","                        log_message(f\"    ❌ No suitable format found for table {table_name}\", \"ERROR\")\n","                        restore_details[table_name] = {\"status\": \"failed\", \"reason\": \"no_suitable_format\"}\n","                        tables_failed += 1\n","                        continue\n","                    \n","                    # Write to Delta table\n","                    if df is not None:\n","                        df.write.mode(\"overwrite\").format(\"delta\").save(target_table_path)\n","                        row_count = df.count()\n","                        \n","                        tables_restored += 1\n","                        restore_details[table_name] = {\n","                            \"status\": \"success\", \n","                            \"format_used\": restore_format,\n","                            \"row_count\": row_count\n","                        }\n","                        \n","                        log_message(f\"    ✅ Restored table {table_name} ({row_count:,} rows)\", \"INFO\")\n","                    \n","                except Exception as table_error:\n","                    log_message(f\"    ❌ Failed to restore table {table_name}: {str(table_error)}\", \"ERROR\")\n","                    restore_details[table_name] = {\"status\": \"failed\", \"error\": str(table_error)}\n","                    tables_failed += 1\n","        \n","        log_message(f\"📊 Table restore completed: {tables_restored} restored, {tables_failed} failed\", \"INFO\")\n","        \n","        return {\n","            \"success\": True,\n","            \"tables_restored\": tables_restored,\n","            \"tables_failed\": tables_failed,\n","            \"restore_details\": restore_details\n","        }\n","        \n","    except Exception as e:\n","        log_message(f\"Table restore failed: {str(e)}\", \"ERROR\")\n","        return {\"success\": False, \"error\": str(e)}\n","\n","def determine_table_restore_format(table_info, preference):\n","    \"\"\"Determine the best format to use for table restore\"\"\"\n","    if preference == \"auto\":\n","        # Prefer Parquet if available, fallback to CSV\n","        if table_info[\"has_parquet\"]:\n","            return \"parquet\"\n","        elif table_info[\"has_csv\"]:\n","            return \"csv\"\n","        else:\n","            return \"none\"\n","    elif preference == \"parquet\":\n","        return \"parquet\" if table_info[\"has_parquet\"] else \"csv\"\n","    elif preference == \"csv\":\n","        return \"csv\" if table_info[\"has_csv\"] else \"parquet\"\n","    else:\n","        return \"auto\"\n","\n","print(\"✅ FIXED Table restore functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 7: File Restore Functions\n","# ============================================================================\n","\n","def restore_files_from_zip(zip_contents, target_files_path):\n","    \"\"\"Restore files from ZIP backup with original format preservation\"\"\"\n","    try:\n","        if not restore_files:\n","            log_message(\"⏭️  File restore skipped (restore_files=False)\", \"INFO\")\n","            return {\"skipped\": True, \"reason\": \"restore_files=False\"}\n","        \n","        files_to_restore = zip_contents[\"files\"]\n","        \n","        # Filter specific files if requested\n","        if restore_specific_files:\n","            import fnmatch\n","            filtered_files = []\n","            for file_info in files_to_restore:\n","                for pattern in restore_specific_files:\n","                    if fnmatch.fnmatch(file_info[\"relative_path\"], pattern):\n","                        filtered_files.append(file_info)\n","                        break\n","            files_to_restore = filtered_files\n","            log_message(f\"🎯 Restoring files matching patterns: {restore_specific_files}\", \"INFO\")\n","        \n","        if not files_to_restore:\n","            return {\"success\": True, \"files_restored\": 0, \"message\": \"No files to restore\"}\n","        \n","        log_message(f\"📁 Starting file restore for {len(files_to_restore)} files...\", \"INFO\")\n","        \n","        zip_binary = zip_contents[\"zip_binary\"]\n","        files_restored = 0\n","        files_failed = 0\n","        files_skipped = 0\n","        restore_details = {}\n","        total_size_restored = 0\n","        \n","        with zipfile.ZipFile(BytesIO(zip_binary), 'r') as zip_file:\n","            \n","            for file_info in files_to_restore:\n","                try:\n","                    relative_path = file_info[\"relative_path\"]\n","                    file_name = file_info[\"name\"]\n","                    \n","                    log_message(f\"  📄 Restoring file: {relative_path}\", \"INFO\")\n","                    \n","                    if dry_run:\n","                        log_message(f\"    🔍 DRY RUN: Would restore file {relative_path}\", \"INFO\")\n","                        restore_details[relative_path] = {\"status\": \"dry_run\", \"type\": file_info[\"type\"]}\n","                        continue\n","                    \n","                    # Check if target file exists\n","                    target_file_path = f\"{target_files_path}/{relative_path}\"\n","                    file_exists = False\n","                    \n","                    if fabric_utils_available:\n","                        try:\n","                            fabric_utils.fs.head(target_file_path, 1)\n","                            file_exists = True\n","                        except:\n","                            file_exists = False\n","                    \n","                    if file_exists and not overwrite_existing:\n","                        log_message(f\"    ⚠️  File {relative_path} exists, skipping (overwrite_existing=False)\", \"WARNING\")\n","                        restore_details[relative_path] = {\"status\": \"skipped\", \"reason\": \"exists\"}\n","                        files_skipped += 1\n","                        continue\n","                    \n","                    # Read file content from ZIP (preserves original format)\n","                    file_content = zip_file.read(file_info[\"zip_path\"])\n","                    file_size = len(file_content)\n","                    \n","                    # Restore file with original format\n","                    if fabric_utils_available:\n","                        # Method 1: Use Fabric utilities for direct file write\n","                        try:\n","                            # Create directory if needed\n","                            target_dir = \"/\".join(target_file_path.split(\"/\")[:-1])\n","                            try:\n","                                fabric_utils.fs.mkdirs(target_dir)\n","                            except:\n","                                pass  # Directory might already exist\n","                            \n","                            # Write to temp file first\n","                            temp_file_path = f\"/tmp/restore_{uuid.uuid4().hex[:8]}_{file_name}\"\n","                            with open(temp_file_path, \"wb\") as f:\n","                                f.write(file_content)\n","                            \n","                            # Copy to target location\n","                            fabric_utils.fs.cp(f\"file:{temp_file_path}\", target_file_path)\n","                            \n","                            # Clean up temp file\n","                            os.remove(temp_file_path)\n","                            \n","                            log_message(f\"    ✅ Restored file: {relative_path} ({file_size/1024:.1f} KB)\", \"INFO\")\n","                            files_restored += 1\n","                            total_size_restored += file_size\n","                            \n","                            restore_details[relative_path] = {\n","                                \"status\": \"success\",\n","                                \"method\": \"fabric_utils\",\n","                                \"size_bytes\": file_size,\n","                                \"type\": file_info[\"type\"]\n","                            }\n","                            \n","                        except Exception as fabric_error:\n","                            log_message(f\"    ⚠️  Fabric utils failed for {relative_path}, trying Spark method: {str(fabric_error)}\", \"WARNING\")\n","                            \n","                            # Fallback to Spark method\n","                            success = restore_file_with_spark(file_content, target_file_path, relative_path, file_info)\n","                            if success:\n","                                files_restored += 1\n","                                total_size_restored += file_size\n","                                restore_details[relative_path] = {\n","                                    \"status\": \"success\",\n","                                    \"method\": \"spark_fallback\",\n","                                    \"size_bytes\": file_size,\n","                                    \"type\": file_info[\"type\"],\n","                                    \"note\": \"Stored as Delta table due to Fabric utils limitation\"\n","                                }\n","                            else:\n","                                files_failed += 1\n","                                restore_details[relative_path] = {\n","                                    \"status\": \"failed\",\n","                                    \"error\": \"Both Fabric utils and Spark methods failed\"\n","                                }\n","                    \n","                    else:\n","                        # Method 2: Spark-only method (stores as Delta)\n","                        success = restore_file_with_spark(file_content, target_file_path, relative_path, file_info)\n","                        if success:\n","                            files_restored += 1\n","                            total_size_restored += file_size\n","                            restore_details[relative_path] = {\n","                                \"status\": \"success\",\n","                                \"method\": \"spark_only\",\n","                                \"size_bytes\": file_size,\n","                                \"type\": file_info[\"type\"],\n","                                \"note\": \"Stored as Delta table (Fabric utils not available)\"\n","                            }\n","                        else:\n","                            files_failed += 1\n","                            restore_details[relative_path] = {\n","                                \"status\": \"failed\",\n","                                \"error\": \"Spark method failed\"\n","                            }\n","                    \n","                except Exception as file_error:\n","                    log_message(f\"    ❌ Failed to restore file {relative_path}: {str(file_error)}\", \"ERROR\")\n","                    restore_details[relative_path] = {\"status\": \"failed\", \"error\": str(file_error)}\n","                    files_failed += 1\n","        \n","        log_message(f\"📁 File restore completed: {files_restored} restored, {files_skipped} skipped, {files_failed} failed\", \"INFO\")\n","        log_message(f\"📊 Total size restored: {total_size_restored/1024/1024:.2f} MB\", \"INFO\")\n","        \n","        return {\n","            \"success\": True,\n","            \"files_restored\": files_restored,\n","            \"files_skipped\": files_skipped,\n","            \"files_failed\": files_failed,\n","            \"total_size_mb\": total_size_restored / 1024 / 1024,\n","            \"restore_details\": restore_details\n","        }\n","        \n","    except Exception as e:\n","        log_message(f\"File restore failed: {str(e)}\", \"ERROR\")\n","        return {\"success\": False, \"error\": str(e)}\n","\n","def restore_file_with_spark(file_content, target_file_path, relative_path, file_info):\n","    \"\"\"Restore file using Spark (fallback method)\"\"\"\n","    try:\n","        # Create a DataFrame with the file content\n","        file_df = spark.createDataFrame([{\n","            \"original_path\": relative_path,\n","            \"file_name\": file_info[\"name\"],\n","            \"file_type\": file_info[\"type\"],\n","            \"content\": file_content,\n","            \"size_bytes\": len(file_content),\n","            \"restored_timestamp\": datetime.datetime.now().isoformat(),\n","            \"restoration_note\": \"File content preserved in binary format within Delta table\"\n","        }])\n","        \n","        # Save as Delta table\n","        file_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{target_file_path}_restored\")\n","        \n","        log_message(f\"    ✅ Restored file via Spark: {relative_path} (stored as Delta)\", \"INFO\")\n","        return True\n","        \n","    except Exception as e:\n","        log_message(f\"    ❌ Spark file restore failed for {relative_path}: {str(e)}\", \"ERROR\")\n","        return False\n","\n","print(\"✅ File restore functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 8: Verification Functions\n","# ============================================================================\n","\n","def verify_restored_data(target_paths, zip_contents, table_restore_result, file_restore_result):\n","    \"\"\"Verify that restored data matches the backup\"\"\"\n","    try:\n","        if not verify_restore:\n","            log_message(\"⏭️  Verification skipped (verify_restore=False)\", \"INFO\")\n","            return {\"skipped\": True}\n","        \n","        log_message(\"🔍 Starting data verification...\", \"INFO\")\n","        verification_results = {\n","            \"tables\": {},\n","            \"files\": {},\n","            \"overall_success\": True\n","        }\n","        \n","        # Verify tables\n","        if restore_tables and table_restore_result.get(\"tables_restored\", 0) > 0:\n","            log_message(\"  📊 Verifying restored tables...\", \"INFO\")\n","            \n","            for table_name, restore_info in table_restore_result.get(\"restore_details\", {}).items():\n","                if restore_info.get(\"status\") == \"success\":\n","                    try:\n","                        # Check if table is readable\n","                        table_path = f\"{target_paths['tables']}/{table_name}\"\n","                        df = spark.read.format(\"delta\").load(table_path)\n","                        actual_count = df.count()\n","                        expected_count = restore_info.get(\"row_count\", 0)\n","                        \n","                        tables_match = actual_count == expected_count\n","                        verification_results[\"tables\"][table_name] = {\n","                            \"verified\": True,\n","                            \"data_matches\": tables_match,\n","                            \"actual_rows\": actual_count,\n","                            \"expected_rows\": expected_count\n","                        }\n","                        \n","                        if tables_match:\n","                            log_message(f\"    ✅ Table {table_name}: {actual_count:,} rows verified\", \"INFO\")\n","                        else:\n","                            log_message(f\"    ⚠️  Table {table_name}: Row count mismatch (expected {expected_count:,}, got {actual_count:,})\", \"WARNING\")\n","                            verification_results[\"overall_success\"] = False\n","                        \n","                    except Exception as table_verify_error:\n","                        log_message(f\"    ❌ Failed to verify table {table_name}: {str(table_verify_error)}\", \"ERROR\")\n","                        verification_results[\"tables\"][table_name] = {\n","                            \"verified\": False,\n","                            \"error\": str(table_verify_error)\n","                        }\n","                        verification_results[\"overall_success\"] = False\n","        \n","        # Verify files\n","        if restore_files and file_restore_result.get(\"files_restored\", 0) > 0:\n","            log_message(\"  📁 Verifying restored files...\", \"INFO\")\n","            \n","            for file_path, restore_info in file_restore_result.get(\"restore_details\", {}).items():\n","                if restore_info.get(\"status\") == \"success\":\n","                    try:\n","                        # Check if file exists and is accessible\n","                        target_file_path = f\"{target_paths['files']}/{file_path}\"\n","                        file_exists = False\n","                        \n","                        if fabric_utils_available and restore_info.get(\"method\") == \"fabric_utils\":\n","                            try:\n","                                fabric_utils.fs.head(target_file_path, 1)\n","                                file_exists = True\n","                            except:\n","                                file_exists = False\n","                        else:\n","                            # Check Delta table\n","                            try:\n","                                file_df = spark.read.format(\"delta\").load(f\"{target_file_path}_restored\")\n","                                file_df.limit(1).collect()\n","                                file_exists = True\n","                            except:\n","                                file_exists = False\n","                        \n","                        verification_results[\"files\"][file_path] = {\n","                            \"verified\": True,\n","                            \"file_exists\": file_exists,\n","                            \"restore_method\": restore_info.get(\"method\", \"unknown\")\n","                        }\n","                        \n","                        if file_exists:\n","                            log_message(f\"    ✅ File {file_path}: Accessible\", \"INFO\")\n","                        else:\n","                            log_message(f\"    ⚠️  File {file_path}: Not accessible after restore\", \"WARNING\")\n","                            verification_results[\"overall_success\"] = False\n","                        \n","                    except Exception as file_verify_error:\n","                        log_message(f\"    ❌ Failed to verify file {file_path}: {str(file_verify_error)}\", \"ERROR\")\n","                        verification_results[\"files\"][file_path] = {\n","                            \"verified\": False,\n","                            \"error\": str(file_verify_error)\n","                        }\n","                        verification_results[\"overall_success\"] = False\n","        \n","        tables_verified = len([t for t in verification_results[\"tables\"].values() if t.get(\"verified\")])\n","        files_verified = len([f for f in verification_results[\"files\"].values() if f.get(\"verified\")])\n","        \n","        log_message(f\"🔍 Verification complete: {tables_verified} tables, {files_verified} files verified\", \"INFO\")\n","        \n","        if verification_results[\"overall_success\"]:\n","            log_message(\"✅ All verification checks passed\", \"INFO\")\n","        else:\n","            log_message(\"⚠️  Some verification checks failed - see details above\", \"WARNING\")\n","        \n","        return verification_results\n","        \n","    except Exception as e:\n","        log_message(f\"Verification failed: {str(e)}\", \"ERROR\")\n","        return {\"success\": False, \"error\": str(e)}\n","\n","print(\"✅ Verification functions defined successfully\")\n","\n","# CELL 9: Fixed Main Restore Execution (replace the manifest creation part)\n","# ============================================================================\n","\n","try:\n","    start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    log_message(\"🔄 Starting Microsoft Fabric Lakehouse COMPLETE RESTORE\", \"INFO\")\n","    log_message(\"📋 Restoring Tables and Files from unified backup\", \"INFO\")\n","    \n","    # ... (keep all the existing code until the manifest creation part)\n","    \n","    # Execute restore (keep existing code)\n","    table_restore_result = restore_tables_from_zip(zip_contents, target_paths['tables'])\n","    file_restore_result = restore_files_from_zip(zip_contents, target_paths['files'])\n","    verification_result = verify_restored_data(target_paths, zip_contents, table_restore_result, file_restore_result)\n","    \n","    # Record end time\n","    end_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    \n","    # Create restore manifest - FIXED VERSION\n","    restore_manifest_simple = {\n","        \"restore_id\": str(uuid.uuid4())[:8],\n","        \"restore_timestamp\": get_current_timestamp(),\n","        \"source_backup_path\": backup_source_path,\n","        \"target_lakehouse\": target_lakehouse_name,\n","        \"restore_type\": \"complete_lakehouse_restore\",\n","        \"fabric_utils_available\": fabric_utils_available,\n","        \"fabric_utils_name\": utils_name,\n","        \"dry_run\": dry_run,\n","        \"start_time\": start_time,\n","        \"end_time\": end_time,\n","        \"duration_seconds\": (datetime.datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\") - \n","                             datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds(),\n","        \"backup_type\": backup_info.get(\"backup_type\", \"unknown\"),\n","        \"restore_method\": backup_info.get(\"restore_method\", \"unknown\"),\n","        \"tables_restored\": table_restore_result.get(\"tables_restored\", 0),\n","        \"tables_failed\": table_restore_result.get(\"tables_failed\", 0),\n","        \"files_restored\": file_restore_result.get(\"files_restored\", 0),\n","        \"files_failed\": file_restore_result.get(\"files_failed\", 0),\n","        \"verification_passed\": verification_result.get(\"overall_success\", False)\n","    }\n","    \n","    if not dry_run:\n","        # Save restore manifest - FIXED VERSION\n","        try:\n","            manifest_df = spark.createDataFrame([restore_manifest_simple])\n","            manifest_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{target_paths['files']}/_restore_manifest_{get_current_timestamp()}\")\n","            log_message(\"📝 Restore manifest saved successfully\", \"INFO\")\n","        except Exception as manifest_error:\n","            log_message(f\"Warning: Could not save restore manifest: {str(manifest_error)}\", \"WARNING\")\n","    \n","    # Write logs\n","    if enable_detailed_logging and log_entries and not dry_run:\n","        try:\n","            log_schema = StructType([\n","                StructField(\"timestamp\", StringType(), True),\n","                StructField(\"level\", StringType(), True),\n","                StructField(\"message\", StringType(), True)\n","            ])\n","            \n","            log_rows = [(entry[\"timestamp\"], entry[\"level\"], entry[\"message\"]) for entry in log_entries]\n","            log_df = spark.createDataFrame(log_rows, log_schema)\n","            log_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{target_paths['files']}/_restore_logs_{get_current_timestamp()}\")\n","            log_message(\"📝 Restore logs written to target lakehouse\", \"INFO\")\n","        except Exception as e:\n","            log_message(f\"Error writing restore logs: {str(e)}\", \"WARNING\")\n","    \n","    # Final summary\n","    duration_seconds = (datetime.datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\") - \n","                        datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n","    \n","    log_message(\"\", \"INFO\")\n","    log_message(\"🎉 \" + \"=\" * 58, \"INFO\")\n","    log_message(\"🎉 LAKEHOUSE RESTORE COMPLETED!\", \"INFO\")\n","    log_message(\"🎉 \" + \"=\" * 58, \"INFO\")\n","    log_message(f\"✅ Restore finished at: {end_time}\", \"INFO\")\n","    log_message(f\"📂 Target lakehouse: {target_lakehouse_name}\", \"INFO\")\n","    log_message(f\"💾 Source backup: {backup_source_path}\", \"INFO\")\n","    \n","    if dry_run:\n","        log_message(\"🔍 DRY RUN COMPLETED - No actual changes made\", \"INFO\")\n","    else:\n","        tables_restored = table_restore_result.get(\"tables_restored\", 0)\n","        files_restored = file_restore_result.get(\"files_restored\", 0)\n","        \n","        log_message(f\"📊 Tables restored: {tables_restored}\", \"INFO\")\n","        log_message(f\"📁 Files restored: {files_restored}\", \"INFO\")\n","        \n","        if 'total_size_mb' in file_restore_result:\n","            log_message(f\"📦 Data size restored: {file_restore_result['total_size_mb']:.2f} MB\", \"INFO\")\n","        \n","        if verification_result.get(\"overall_success\"):\n","            log_message(\"✅ Verification: All checks passed\", \"INFO\")\n","        elif verification_result.get(\"skipped\"):\n","            log_message(\"⏭️  Verification: Skipped\", \"INFO\")\n","        else:\n","            log_message(\"⚠️  Verification: Some checks failed\", \"WARNING\")\n","    \n","    log_message(f\"⏱️  Duration: {duration_seconds:.2f} seconds\", \"INFO\")\n","    log_message(\"=\" * 60, \"INFO\")\n","    \n","    # Final result\n","    final_result = {\n","        \"status\": \"success\",\n","        \"restore_type\": \"complete_lakehouse\",\n","        \"target_lakehouse\": target_lakehouse_name,\n","        \"source_backup\": backup_source_path,\n","        \"duration_seconds\": duration_seconds,\n","        \"dry_run\": dry_run,\n","        \"tables_restored\": table_restore_result.get(\"tables_restored\", 0),\n","        \"files_restored\": file_restore_result.get(\"files_restored\", 0),\n","        \"verification_passed\": verification_result.get(\"overall_success\", False),\n","        \"backup_type\": backup_info[\"backup_type\"],\n","        \"restore_method\": backup_info[\"restore_method\"]\n","    }\n","    \n","    print(f\"\\n🎉 LAKEHOUSE RESTORE RESULT:\")\n","    print(json.dumps(final_result, indent=2, default=str))\n","    \n","except Exception as e:\n","    log_message(f\"💥 Lakehouse restore FAILED: {str(e)}\", \"ERROR\")\n","    import traceback\n","    log_message(f\"Full error trace: {traceback.format_exc()}\", \"ERROR\")\n","    \n","    failure_result = {\n","        \"status\": \"failed\",\n","        \"error\": str(e),\n","        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    }\n","    \n","    print(\"\\n💥 LAKEHOUSE RESTORE RESULT:\")\n","    print(json.dumps(failure_result, indent=2))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"cdb7285e-ee2d-4802-b96f-d1eac5fce30b","normalized_state":"finished","queued_time":"2025-07-03T20:53:05.2266807Z","session_start_time":null,"execution_start_time":"2025-07-03T20:53:05.2326304Z","execution_finish_time":"2025-07-03T20:53:40.5044088Z","parent_msg_id":"99ab600d-e069-4e86-917a-39538dd5e46a"},"text/plain":"StatementMeta(, cdb7285e-ee2d-4802-b96f-d1eac5fce30b, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["📚 Libraries imported successfully\n🔧 Fabric utilities available: True (mssparkutils)\n🔄 Lakehouse restore process initiated at: 2025-07-03 20:53:05.483703\n✅ Core helper functions defined successfully\n✅ Path construction functions defined successfully\n✅ Backup analysis and ZIP reading functions defined successfully\n✅ FIXED Table restore functions defined successfully\n✅ File restore functions defined successfully\n✅ Verification functions defined successfully\n[2025-07-03 20:53:05] [INFO] 🔄 Starting Microsoft Fabric Lakehouse COMPLETE RESTORE\n[2025-07-03 20:53:05] [INFO] 📋 Restoring Tables and Files from unified backup\n[2025-07-03 20:53:05] [INFO] 📊 Starting table restore for 2 tables...\n[2025-07-03 20:53:05] [INFO]   📋 Restoring table: customers\n[2025-07-03 20:53:22] [WARNING]     ⚠️  Table customers exists, skipping (overwrite_existing=False)\n[2025-07-03 20:53:22] [INFO]   📋 Restoring table: products\n[2025-07-03 20:53:25] [WARNING]     ⚠️  Table products exists, skipping (overwrite_existing=False)\n[2025-07-03 20:53:25] [INFO] 📊 Table restore completed: 0 restored, 0 failed\n[2025-07-03 20:53:25] [INFO] 📁 Starting file restore for 1 files...\n[2025-07-03 20:53:25] [INFO]   📄 Restoring file: car_manufacturing_data_pipeline.svg\n[2025-07-03 20:53:25] [WARNING]     ⚠️  File car_manufacturing_data_pipeline.svg exists, skipping (overwrite_existing=False)\n[2025-07-03 20:53:25] [INFO] 📁 File restore completed: 0 restored, 1 skipped, 0 failed\n[2025-07-03 20:53:25] [INFO] 📊 Total size restored: 0.00 MB\n[2025-07-03 20:53:25] [INFO] 🔍 Starting data verification...\n[2025-07-03 20:53:25] [INFO] 🔍 Verification complete: 0 tables, 0 files verified\n[2025-07-03 20:53:25] [INFO] ✅ All verification checks passed\n[2025-07-03 20:53:37] [INFO] 📝 Restore manifest saved successfully\n[2025-07-03 20:53:39] [INFO] 📝 Restore logs written to target lakehouse\n[2025-07-03 20:53:39] [INFO] \n[2025-07-03 20:53:39] [INFO] 🎉 ==========================================================\n[2025-07-03 20:53:39] [INFO] 🎉 LAKEHOUSE RESTORE COMPLETED!\n[2025-07-03 20:53:39] [INFO] 🎉 ==========================================================\n[2025-07-03 20:53:39] [INFO] ✅ Restore finished at: 2025-07-03 20:53:25\n[2025-07-03 20:53:39] [INFO] 📂 Target lakehouse: lh_msft_bae_restore\n[2025-07-03 20:53:39] [INFO] 💾 Source backup: abfss://WS-MSFT-BAE-DEMO-SBX@onelake.dfs.fabric.microsoft.com/lh_msft_bae_backup.Lakehouse/Files/complete_backup_2025-07-03_20-41-04_64af2286\n[2025-07-03 20:53:39] [INFO] 📊 Tables restored: 0\n[2025-07-03 20:53:39] [INFO] 📁 Files restored: 0\n[2025-07-03 20:53:39] [INFO] 📦 Data size restored: 0.00 MB\n[2025-07-03 20:53:39] [INFO] ✅ Verification: All checks passed\n[2025-07-03 20:53:39] [INFO] ⏱️  Duration: 20.00 seconds\n[2025-07-03 20:53:39] [INFO] ============================================================\n\n🎉 LAKEHOUSE RESTORE RESULT:\n{\n  \"status\": \"success\",\n  \"restore_type\": \"complete_lakehouse\",\n  \"target_lakehouse\": \"lh_msft_bae_restore\",\n  \"source_backup\": \"abfss://WS-MSFT-BAE-DEMO-SBX@onelake.dfs.fabric.microsoft.com/lh_msft_bae_backup.Lakehouse/Files/complete_backup_2025-07-03_20-41-04_64af2286\",\n  \"duration_seconds\": 20.0,\n  \"dry_run\": false,\n  \"tables_restored\": 0,\n  \"files_restored\": 0,\n  \"verification_passed\": true,\n  \"backup_type\": \"zip_file\",\n  \"restore_method\": \"zip_direct\"\n}\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86bac095-efd7-4563-8682-1772c558b430"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"kernel_info":{"name":"synapse_pyspark"}},"nbformat":4,"nbformat_minor":5}