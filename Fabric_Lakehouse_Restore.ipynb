{"cells":[{"cell_type":"code","source":["# Microsoft Fabric Lakehouse Complete Restore Notebook\n","# Restores Tables and Files from unified backup ZIP\n","\n","# ============================================================================\n","# CELL 1: Restore Configuration Parameters (Parameterized Cell)\n","# ============================================================================\n","\n","# Parameters - Configure these values or override when scheduling\n","backup_source_type = \"lakehouse\"  # Options: \"storage_account\", \"lakehouse\", \"adls\", \"local_file\"\n","backup_storage_account = \"\"  # Required for storage_account type\n","backup_container = \"lakehouse-backups\"  # Container name for storage account\n","backup_lakehouse_name = \"lh_msft_bae_backup\"  # Required for lakehouse type\n","backup_workspace_id = \"WS-MSFT-BAE-DEMO-SBX\"  # REQUIRED: Backup workspace ID\n","backup_adls_account = \"\"  # Required for adls type\n","backup_adls_container = \"\"  # Required for adls type\n","backup_path = \"complete_backup_2025-07-03_20-41-04_64af2286\"  # REQUIRED: Path to the backup (auto-detect if empty)\n","\n","# Restore target configuration\n","target_lakehouse_name = \"lh_msft_bae_restore\"  # REQUIRED: Target lakehouse name\n","target_workspace_id = \"WS-MSFT-BAE-DEMO-SBX\"  # REQUIRED: Target workspace ID\n","\n","# Restore options\n","restore_tables = True  # Restore tables to Tables directory\n","restore_files = True   # Restore files to Files directory\n","table_format_preference = \"parquet\"  # Options: \"parquet\", \"csv\", \"auto\"\n","overwrite_existing = False  # Overwrite existing tables/files\n","create_target_lakehouse = False  # Create target lakehouse if it doesn't exist\n","restore_method = \"auto\"  # Options: \"auto\", \"zip_direct\", \"zip_from_delta\"\n","\n","# Advanced options\n","verify_restore = True  # Verify restored data\n","enable_detailed_logging = True  # Enable detailed logging\n","dry_run = False  # Show what would be restored without actually restoring\n","restore_specific_tables = []  # List of specific table names to restore (empty = all)\n","restore_specific_files = []  # List of specific file patterns to restore (empty = all)\n","use_managed_identity = True  # Use managed identity for external storage\n","\n","# ============================================================================\n","# CELL 2: Import Required Libraries and Initialize\n","# ============================================================================\n","\n","import os\n","import json\n","import datetime\n","import uuid\n","import zipfile\n","from io import BytesIO\n","from pyspark.sql.functions import lit, col, current_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, LongType, BooleanType, BinaryType\n","import time\n","import tempfile\n","\n","# Initialize global variables\n","log_entries = []\n","restore_start_time = datetime.datetime.now()\n","\n","# Check for Fabric utilities availability\n","fabric_utils_available = False\n","try:\n","    if 'mssparkutils' in dir():\n","        fabric_utils = mssparkutils\n","        fabric_utils_available = True\n","        utils_name = \"mssparkutils\"\n","    elif 'notebookutils' in dir():\n","        fabric_utils = notebookutils\n","        fabric_utils_available = True\n","        utils_name = \"notebookutils\"\n","    else:\n","        try:\n","            import notebookutils as fabric_utils\n","            fabric_utils_available = True\n","            utils_name = \"notebookutils\"\n","        except:\n","            try:\n","                import mssparkutils as fabric_utils\n","                fabric_utils_available = True\n","                utils_name = \"mssparkutils\"\n","            except:\n","                fabric_utils_available = False\n","                utils_name = \"none\"\n","except:\n","    fabric_utils_available = False\n","    utils_name = \"none\"\n","\n","print(\"üìö Libraries imported successfully\")\n","print(f\"üîß Fabric utilities available: {fabric_utils_available} ({utils_name})\")\n","print(f\"üîÑ Lakehouse restore process initiated at: {restore_start_time}\")\n","\n","# ============================================================================\n","# CELL 3: Core Helper Functions\n","# ============================================================================\n","\n","def get_current_timestamp():\n","    \"\"\"Return current timestamp in a standardized format\"\"\"\n","    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","def log_message(message, level=\"INFO\"):\n","    \"\"\"Log a message with timestamp and level\"\"\"\n","    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    print(f\"[{timestamp}] [{level}] {message}\")\n","    \n","    if enable_detailed_logging and level != \"DEBUG\":\n","        log_entries.append({\n","            \"timestamp\": timestamp,\n","            \"level\": level,\n","            \"message\": message\n","        })\n","\n","def validate_parameters():\n","    \"\"\"Validate that required parameters are provided\"\"\"\n","    if not target_lakehouse_name:\n","        raise ValueError(\"Target Lakehouse Name is required\")\n","    \n","    if not backup_path:\n","        raise ValueError(\"Backup path is required\")\n","    \n","    if backup_source_type == \"storage_account\" and not backup_storage_account:\n","        raise ValueError(\"Backup Storage Account is required for storage_account backup type\")\n","    elif backup_source_type == \"lakehouse\" and not backup_lakehouse_name:\n","        raise ValueError(\"Backup Lakehouse Name is required for lakehouse backup type\")\n","    elif backup_source_type == \"adls\" and (not backup_adls_account or not backup_adls_container):\n","        raise ValueError(\"Backup ADLS Account and Container are required for adls backup type\")\n","\n","def get_workspace_id():\n","    \"\"\"Get the current workspace ID\"\"\"\n","    try:\n","        methods = [\n","            (\"spark.fabric.workspaceId\", lambda: spark.conf.get(\"spark.fabric.workspaceId\", None)),\n","            (\"spark.sql.hive.metastore.warehouse.dir\", lambda: extract_workspace_from_warehouse_dir()),\n","            (\"WORKSPACE_ID env var\", lambda: os.environ.get(\"WORKSPACE_ID\", None)),\n","        ]\n","        \n","        for method_name, method_func in methods:\n","            try:\n","                workspace_id = method_func()\n","                if workspace_id:\n","                    log_message(f\"Found workspace ID using {method_name}: {workspace_id}\", \"DEBUG\")\n","                    return workspace_id\n","            except Exception as e:\n","                log_message(f\"Method {method_name} failed: {str(e)}\", \"DEBUG\")\n","        \n","        return None\n","        \n","    except Exception as e:\n","        log_message(f\"Error getting workspace ID: {str(e)}\", \"WARNING\")\n","        return None\n","\n","def extract_workspace_from_warehouse_dir():\n","    \"\"\"Extract workspace ID from warehouse directory path\"\"\"\n","    try:\n","        warehouse_dir = spark.conf.get(\"spark.sql.hive.metastore.warehouse.dir\", \"\")\n","        if \"onelake.dfs.fabric.microsoft.com\" in warehouse_dir:\n","            import re\n","            match = re.search(r'abfss://([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})@onelake', warehouse_dir)\n","            if match:\n","                return match.group(1)\n","        return None\n","    except Exception:\n","        return None\n","\n","def setup_external_storage_auth():\n","    \"\"\"Setup authentication for external storage if needed\"\"\"\n","    if backup_source_type in [\"storage_account\", \"adls\"] and use_managed_identity:\n","        log_message(\"Configuring managed identity for external storage authentication\", \"INFO\")\n","        \n","        try:\n","            if backup_source_type == \"storage_account\":\n","                account_name = backup_storage_account\n","                spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n","                spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n","            elif backup_source_type == \"adls\":\n","                account_name = backup_adls_account\n","                spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n","                spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n","            \n","            log_message(\"External storage authentication configured\", \"INFO\")\n","        except Exception as e:\n","            log_message(f\"Warning: Could not configure external storage auth: {str(e)}\", \"WARNING\")\n","\n","print(\"‚úÖ Core helper functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 4: Path Construction Functions\n","# ============================================================================\n","\n","def get_backup_source_path():\n","    \"\"\"Construct path to the backup source based on the backup type\"\"\"\n","    if backup_source_type == \"storage_account\":\n","        return f\"abfss://{backup_container}@{backup_storage_account}.dfs.core.windows.net/{backup_path}\"\n","    \n","    elif backup_source_type == \"lakehouse\":\n","        if backup_workspace_id and backup_workspace_id.strip():\n","            workspace_id = backup_workspace_id\n","        else:\n","            workspace_id = get_workspace_id()\n","        \n","        if workspace_id:\n","            return f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{backup_lakehouse_name}.Lakehouse/Files/{backup_path}\"\n","        else:\n","            raise ValueError(\"Workspace ID is required for OneLake access. Please provide backup_workspace_id parameter.\")\n","    \n","    elif backup_source_type == \"adls\":\n","        return f\"abfss://{backup_adls_container}@{backup_adls_account}.dfs.core.windows.net/{backup_path}\"\n","    \n","    elif backup_source_type == \"local_file\":\n","        return backup_path\n","    \n","    else:\n","        raise ValueError(f\"Invalid backup source type: {backup_source_type}\")\n","\n","def get_target_paths():\n","    \"\"\"Construct paths to the target lakehouse Tables and Files directories\"\"\"\n","    if target_workspace_id and target_workspace_id.strip():\n","        workspace_id = target_workspace_id\n","    else:\n","        workspace_id = get_workspace_id()\n","    \n","    if workspace_id:\n","        base_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{target_lakehouse_name}.Lakehouse\"\n","        return {\n","            \"tables\": f\"{base_path}/Tables\",\n","            \"files\": f\"{base_path}/Files\"\n","        }\n","    else:\n","        raise ValueError(\"Workspace ID is required for OneLake access. Please provide target_workspace_id parameter.\")\n","\n","print(\"‚úÖ Path construction functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 5: Backup Analysis and ZIP Reading Functions\n","# ============================================================================\n","\n","def analyze_backup_source(backup_source_path):\n","    \"\"\"Analyze the backup source and determine the restore method\"\"\"\n","    try:\n","        log_message(\"üîç Analyzing backup source...\", \"INFO\")\n","        \n","        backup_info = {\n","            \"backup_type\": \"unknown\",\n","            \"has_zip_file\": False,\n","            \"has_zip_data\": False,\n","            \"has_manifest\": False,\n","            \"restore_method\": \"unknown\"\n","        }\n","        \n","        # Check for direct ZIP file\n","        try:\n","            if fabric_utils_available:\n","                files = fabric_utils.fs.ls(backup_source_path)\n","                for file in files:\n","                    if file.name.endswith('.zip'):\n","                        backup_info[\"has_zip_file\"] = True\n","                        backup_info[\"zip_file_path\"] = file.path\n","                        log_message(f\"‚úÖ Found ZIP file: {file.name}\", \"INFO\")\n","                        break\n","        except Exception as e:\n","            log_message(f\"Could not list backup directory: {str(e)}\", \"WARNING\")\n","        \n","        # Check for ZIP data in Delta table\n","        try:\n","            zip_data_paths = [\n","                f\"{backup_source_path}/complete_backup_zip_data\",\n","                f\"{backup_source_path}/files_backup_zip_data\",\n","                f\"{backup_source_path}/lakehouse_complete_backup_zip_data\"\n","            ]\n","            \n","            for zip_path in zip_data_paths:\n","                try:\n","                    test_df = spark.read.format(\"delta\").load(zip_path)\n","                    test_df.limit(1).collect()\n","                    backup_info[\"has_zip_data\"] = True\n","                    backup_info[\"zip_data_path\"] = zip_path\n","                    log_message(f\"‚úÖ Found ZIP data in Delta table: {zip_path}\", \"INFO\")\n","                    break\n","                except:\n","                    continue\n","                    \n","        except Exception as e:\n","            log_message(f\"Could not check for ZIP data: {str(e)}\", \"DEBUG\")\n","        \n","        # Check for manifest\n","        try:\n","            manifest_df = spark.read.format(\"delta\").load(f\"{backup_source_path}/_manifest\")\n","            manifest_data = manifest_df.collect()[0]\n","            backup_info[\"has_manifest\"] = True\n","            backup_info[\"manifest_data\"] = manifest_data.asDict()\n","            log_message(\"‚úÖ Found backup manifest\", \"INFO\")\n","        except Exception as e:\n","            log_message(f\"No manifest found: {str(e)}\", \"DEBUG\")\n","        \n","        # Determine restore method\n","        if backup_info[\"has_zip_file\"]:\n","            backup_info[\"restore_method\"] = \"zip_direct\"\n","            backup_info[\"backup_type\"] = \"zip_file\"\n","        elif backup_info[\"has_zip_data\"]:\n","            backup_info[\"restore_method\"] = \"zip_from_delta\"\n","            backup_info[\"backup_type\"] = \"zip_in_delta\"\n","        else:\n","            backup_info[\"restore_method\"] = \"legacy_format\"\n","            backup_info[\"backup_type\"] = \"legacy\"\n","        \n","        log_message(f\"üìã Backup analysis complete: {backup_info['backup_type']} ({backup_info['restore_method']})\", \"INFO\")\n","        return backup_info\n","        \n","    except Exception as e:\n","        log_message(f\"Backup analysis failed: {str(e)}\", \"ERROR\")\n","        return {\"backup_type\": \"unknown\", \"restore_method\": \"unknown\", \"error\": str(e)}\n","\n","def read_zip_contents(backup_info):\n","    \"\"\"Read and analyze ZIP contents\"\"\"\n","    try:\n","        log_message(\"üì¶ Reading ZIP backup contents...\", \"INFO\")\n","        \n","        # Get ZIP binary data\n","        if backup_info[\"restore_method\"] == \"zip_direct\":\n","            # Read ZIP file directly\n","            if fabric_utils_available:\n","                zip_file_path = backup_info[\"zip_file_path\"]\n","                # Copy to local temp for processing\n","                temp_zip_path = f\"/tmp/restore_backup_{uuid.uuid4().hex[:8]}.zip\"\n","                fabric_utils.fs.cp(zip_file_path, f\"file:{temp_zip_path}\")\n","                \n","                with open(temp_zip_path, \"rb\") as f:\n","                    zip_binary = f.read()\n","                \n","                os.remove(temp_zip_path)\n","            else:\n","                raise Exception(\"Cannot read ZIP file without Fabric utilities\")\n","                \n","        elif backup_info[\"restore_method\"] == \"zip_from_delta\":\n","            # Read ZIP from Delta table\n","            zip_data_path = backup_info[\"zip_data_path\"]\n","            zip_df = spark.read.format(\"delta\").load(zip_data_path)\n","            zip_row = zip_df.collect()[0]\n","            zip_binary = zip_row.zip_binary\n","            \n","        else:\n","            raise Exception(f\"Cannot read ZIP for restore method: {backup_info['restore_method']}\")\n","        \n","        # Analyze ZIP contents\n","        zip_contents = {\n","            \"tables\": {},\n","            \"files\": [],\n","            \"metadata\": {},\n","            \"total_size\": len(zip_binary)\n","        }\n","        \n","        with zipfile.ZipFile(BytesIO(zip_binary), 'r') as zip_file:\n","            file_list = zip_file.namelist()\n","            \n","            log_message(f\"üìÅ ZIP contains {len(file_list)} items\", \"INFO\")\n","            \n","            # Parse contents\n","            for file_path in file_list:\n","                if file_path.startswith('tables/'):\n","                    # Parse table structure\n","                    path_parts = file_path.split('/')\n","                    if len(path_parts) >= 3:\n","                        table_name = path_parts[1]\n","                        file_name = path_parts[2]\n","                        \n","                        if table_name not in zip_contents[\"tables\"]:\n","                            zip_contents[\"tables\"][table_name] = {\n","                                \"formats\": [],\n","                                \"has_csv\": False,\n","                                \"has_parquet\": False,\n","                                \"has_schema\": False,\n","                                \"has_metadata\": False\n","                            }\n","                        \n","                        if file_name.endswith('.csv'):\n","                            zip_contents[\"tables\"][table_name][\"has_csv\"] = True\n","                            zip_contents[\"tables\"][table_name][\"formats\"].append(\"csv\")\n","                        elif file_name.endswith('.parquet'):\n","                            zip_contents[\"tables\"][table_name][\"has_parquet\"] = True\n","                            zip_contents[\"tables\"][table_name][\"formats\"].append(\"parquet\")\n","                        elif file_name == 'schema.json':\n","                            zip_contents[\"tables\"][table_name][\"has_schema\"] = True\n","                        elif file_name == 'metadata.json':\n","                            zip_contents[\"tables\"][table_name][\"has_metadata\"] = True\n","                \n","                elif file_path.startswith('files/'):\n","                    # Parse file structure\n","                    relative_path = file_path[6:]  # Remove 'files/' prefix\n","                    if relative_path and not relative_path.endswith('/'):\n","                        zip_contents[\"files\"].append({\n","                            \"zip_path\": file_path,\n","                            \"relative_path\": relative_path,\n","                            \"name\": relative_path.split('/')[-1],\n","                            \"type\": get_file_type(relative_path.split('/')[-1])\n","                        })\n","                \n","                elif file_path.startswith('_backup_info/'):\n","                    # Read metadata\n","                    if file_path.endswith('metadata.json'):\n","                        metadata_content = zip_file.read(file_path)\n","                        zip_contents[\"metadata\"] = json.loads(metadata_content.decode('utf-8'))\n","        \n","        log_message(f\"üìä Found {len(zip_contents['tables'])} tables and {len(zip_contents['files'])} files in ZIP\", \"INFO\")\n","        \n","        # Store ZIP binary for later use\n","        zip_contents[\"zip_binary\"] = zip_binary\n","        \n","        return zip_contents\n","        \n","    except Exception as e:\n","        log_message(f\"Failed to read ZIP contents: {str(e)}\", \"ERROR\")\n","        return None\n","\n","def get_file_type(filename):\n","    \"\"\"Determine file type based on extension\"\"\"\n","    if '.' not in filename:\n","        return \"unknown\"\n","    \n","    extension = filename.split('.')[-1].lower()\n","    \n","    file_types = {\n","        'jpg': 'image', 'jpeg': 'image', 'png': 'image', 'gif': 'image', 'bmp': 'image', 'svg': 'image', 'webp': 'image',\n","        'pdf': 'document', 'doc': 'document', 'docx': 'document', 'txt': 'text', 'rtf': 'document',\n","        'xls': 'spreadsheet', 'xlsx': 'spreadsheet', 'csv': 'data',\n","        'json': 'data', 'xml': 'data', 'yaml': 'data', 'yml': 'data', 'parquet': 'data',\n","        'zip': 'archive', 'rar': 'archive', '7z': 'archive', 'tar': 'archive', 'gz': 'archive',\n","        'py': 'code', 'sql': 'code', 'js': 'code', 'html': 'code', 'css': 'code',\n","        'mp4': 'video', 'avi': 'video', 'mov': 'video', 'mp3': 'audio', 'wav': 'audio'\n","    }\n","    \n","    return file_types.get(extension, 'other')\n","\n","print(\"‚úÖ Backup analysis and ZIP reading functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 6: Fixed Table Restore Functions\n","# ============================================================================\n","\n","def restore_tables_from_zip(zip_contents, target_tables_path):\n","    \"\"\"Restore tables from ZIP backup - FIXED VERSION\"\"\"\n","    try:\n","        if not restore_tables:\n","            log_message(\"‚è≠Ô∏è  Table restore skipped (restore_tables=False)\", \"INFO\")\n","            return {\"skipped\": True, \"reason\": \"restore_tables=False\"}\n","        \n","        tables_to_restore = zip_contents[\"tables\"]\n","        \n","        # Filter specific tables if requested\n","        if restore_specific_tables:\n","            tables_to_restore = {name: info for name, info in tables_to_restore.items() \n","                               if name in restore_specific_tables}\n","            log_message(f\"üéØ Restoring specific tables: {list(tables_to_restore.keys())}\", \"INFO\")\n","        \n","        if not tables_to_restore:\n","            return {\"success\": True, \"tables_restored\": 0, \"message\": \"No tables to restore\"}\n","        \n","        log_message(f\"üìä Starting table restore for {len(tables_to_restore)} tables...\", \"INFO\")\n","        \n","        zip_binary = zip_contents[\"zip_binary\"]\n","        tables_restored = 0\n","        tables_failed = 0\n","        restore_details = {}\n","        \n","        with zipfile.ZipFile(BytesIO(zip_binary), 'r') as zip_file:\n","            \n","            for table_name, table_info in tables_to_restore.items():\n","                try:\n","                    log_message(f\"  üìã Restoring table: {table_name}\", \"INFO\")\n","                    \n","                    if dry_run:\n","                        log_message(f\"    üîç DRY RUN: Would restore table {table_name}\", \"INFO\")\n","                        restore_details[table_name] = {\"status\": \"dry_run\", \"formats\": table_info[\"formats\"]}\n","                        continue\n","                    \n","                    # Check if target table exists\n","                    target_table_path = f\"{target_tables_path}/{table_name}\"\n","                    table_exists = False\n","                    \n","                    try:\n","                        test_df = spark.read.format(\"delta\").load(target_table_path)\n","                        test_df.limit(1).collect()\n","                        table_exists = True\n","                    except:\n","                        table_exists = False\n","                    \n","                    if table_exists and not overwrite_existing:\n","                        log_message(f\"    ‚ö†Ô∏è  Table {table_name} exists, skipping (overwrite_existing=False)\", \"WARNING\")\n","                        restore_details[table_name] = {\"status\": \"skipped\", \"reason\": \"exists\"}\n","                        continue\n","                    \n","                    # Determine which format to use for restore\n","                    restore_format = determine_table_restore_format(table_info, table_format_preference)\n","                    \n","                    df = None\n","                    \n","                    if restore_format == \"parquet\" and table_info[\"has_parquet\"]:\n","                        # Restore from Parquet - FIXED VERSION\n","                        parquet_data = zip_file.read(f\"tables/{table_name}/{table_name}.parquet\")\n","                        \n","                        # Use BytesIO instead of temp file\n","                        parquet_buffer = BytesIO(parquet_data)\n","                        \n","                        # Convert to pandas first, then to Spark\n","                        import pandas as pd\n","                        pandas_df = pd.read_parquet(parquet_buffer)\n","                        df = spark.createDataFrame(pandas_df)\n","                        \n","                        log_message(f\"    ‚úÖ Read from Parquet: {table_name} ({len(pandas_df):,} rows)\", \"INFO\")\n","                        \n","                    elif restore_format == \"csv\" and table_info[\"has_csv\"]:\n","                        # Restore from CSV - FIXED VERSION\n","                        csv_data = zip_file.read(f\"tables/{table_name}/{table_name}.csv\")\n","                        \n","                        # Use BytesIO instead of temp file\n","                        from io import StringIO\n","                        csv_string = csv_data.decode('utf-8')\n","                        csv_buffer = StringIO(csv_string)\n","                        \n","                        # Convert to pandas first, then to Spark\n","                        import pandas as pd\n","                        pandas_df = pd.read_csv(csv_buffer)\n","                        df = spark.createDataFrame(pandas_df)\n","                        \n","                        log_message(f\"    ‚úÖ Read from CSV: {table_name} ({len(pandas_df):,} rows)\", \"INFO\")\n","                        \n","                    else:\n","                        log_message(f\"    ‚ùå No suitable format found for table {table_name}\", \"ERROR\")\n","                        restore_details[table_name] = {\"status\": \"failed\", \"reason\": \"no_suitable_format\"}\n","                        tables_failed += 1\n","                        continue\n","                    \n","                    # Write to Delta table\n","                    if df is not None:\n","                        df.write.mode(\"overwrite\").format(\"delta\").save(target_table_path)\n","                        row_count = df.count()\n","                        \n","                        tables_restored += 1\n","                        restore_details[table_name] = {\n","                            \"status\": \"success\", \n","                            \"format_used\": restore_format,\n","                            \"row_count\": row_count\n","                        }\n","                        \n","                        log_message(f\"    ‚úÖ Restored table {table_name} ({row_count:,} rows)\", \"INFO\")\n","                    \n","                except Exception as table_error:\n","                    log_message(f\"    ‚ùå Failed to restore table {table_name}: {str(table_error)}\", \"ERROR\")\n","                    restore_details[table_name] = {\"status\": \"failed\", \"error\": str(table_error)}\n","                    tables_failed += 1\n","        \n","        log_message(f\"üìä Table restore completed: {tables_restored} restored, {tables_failed} failed\", \"INFO\")\n","        \n","        return {\n","            \"success\": True,\n","            \"tables_restored\": tables_restored,\n","            \"tables_failed\": tables_failed,\n","            \"restore_details\": restore_details\n","        }\n","        \n","    except Exception as e:\n","        log_message(f\"Table restore failed: {str(e)}\", \"ERROR\")\n","        return {\"success\": False, \"error\": str(e)}\n","\n","def determine_table_restore_format(table_info, preference):\n","    \"\"\"Determine the best format to use for table restore\"\"\"\n","    if preference == \"auto\":\n","        # Prefer Parquet if available, fallback to CSV\n","        if table_info[\"has_parquet\"]:\n","            return \"parquet\"\n","        elif table_info[\"has_csv\"]:\n","            return \"csv\"\n","        else:\n","            return \"none\"\n","    elif preference == \"parquet\":\n","        return \"parquet\" if table_info[\"has_parquet\"] else \"csv\"\n","    elif preference == \"csv\":\n","        return \"csv\" if table_info[\"has_csv\"] else \"parquet\"\n","    else:\n","        return \"auto\"\n","\n","print(\"‚úÖ FIXED Table restore functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 7: File Restore Functions\n","# ============================================================================\n","\n","def restore_files_from_zip(zip_contents, target_files_path):\n","    \"\"\"Restore files from ZIP backup with original format preservation\"\"\"\n","    try:\n","        if not restore_files:\n","            log_message(\"‚è≠Ô∏è  File restore skipped (restore_files=False)\", \"INFO\")\n","            return {\"skipped\": True, \"reason\": \"restore_files=False\"}\n","        \n","        files_to_restore = zip_contents[\"files\"]\n","        \n","        # Filter specific files if requested\n","        if restore_specific_files:\n","            import fnmatch\n","            filtered_files = []\n","            for file_info in files_to_restore:\n","                for pattern in restore_specific_files:\n","                    if fnmatch.fnmatch(file_info[\"relative_path\"], pattern):\n","                        filtered_files.append(file_info)\n","                        break\n","            files_to_restore = filtered_files\n","            log_message(f\"üéØ Restoring files matching patterns: {restore_specific_files}\", \"INFO\")\n","        \n","        if not files_to_restore:\n","            return {\"success\": True, \"files_restored\": 0, \"message\": \"No files to restore\"}\n","        \n","        log_message(f\"üìÅ Starting file restore for {len(files_to_restore)} files...\", \"INFO\")\n","        \n","        zip_binary = zip_contents[\"zip_binary\"]\n","        files_restored = 0\n","        files_failed = 0\n","        files_skipped = 0\n","        restore_details = {}\n","        total_size_restored = 0\n","        \n","        with zipfile.ZipFile(BytesIO(zip_binary), 'r') as zip_file:\n","            \n","            for file_info in files_to_restore:\n","                try:\n","                    relative_path = file_info[\"relative_path\"]\n","                    file_name = file_info[\"name\"]\n","                    \n","                    log_message(f\"  üìÑ Restoring file: {relative_path}\", \"INFO\")\n","                    \n","                    if dry_run:\n","                        log_message(f\"    üîç DRY RUN: Would restore file {relative_path}\", \"INFO\")\n","                        restore_details[relative_path] = {\"status\": \"dry_run\", \"type\": file_info[\"type\"]}\n","                        continue\n","                    \n","                    # Check if target file exists\n","                    target_file_path = f\"{target_files_path}/{relative_path}\"\n","                    file_exists = False\n","                    \n","                    if fabric_utils_available:\n","                        try:\n","                            fabric_utils.fs.head(target_file_path, 1)\n","                            file_exists = True\n","                        except:\n","                            file_exists = False\n","                    \n","                    if file_exists and not overwrite_existing:\n","                        log_message(f\"    ‚ö†Ô∏è  File {relative_path} exists, skipping (overwrite_existing=False)\", \"WARNING\")\n","                        restore_details[relative_path] = {\"status\": \"skipped\", \"reason\": \"exists\"}\n","                        files_skipped += 1\n","                        continue\n","                    \n","                    # Read file content from ZIP (preserves original format)\n","                    file_content = zip_file.read(file_info[\"zip_path\"])\n","                    file_size = len(file_content)\n","                    \n","                    # Restore file with original format\n","                    if fabric_utils_available:\n","                        # Method 1: Use Fabric utilities for direct file write\n","                        try:\n","                            # Create directory if needed\n","                            target_dir = \"/\".join(target_file_path.split(\"/\")[:-1])\n","                            try:\n","                                fabric_utils.fs.mkdirs(target_dir)\n","                            except:\n","                                pass  # Directory might already exist\n","                            \n","                            # Write to temp file first\n","                            temp_file_path = f\"/tmp/restore_{uuid.uuid4().hex[:8]}_{file_name}\"\n","                            with open(temp_file_path, \"wb\") as f:\n","                                f.write(file_content)\n","                            \n","                            # Copy to target location\n","                            fabric_utils.fs.cp(f\"file:{temp_file_path}\", target_file_path)\n","                            \n","                            # Clean up temp file\n","                            os.remove(temp_file_path)\n","                            \n","                            log_message(f\"    ‚úÖ Restored file: {relative_path} ({file_size/1024:.1f} KB)\", \"INFO\")\n","                            files_restored += 1\n","                            total_size_restored += file_size\n","                            \n","                            restore_details[relative_path] = {\n","                                \"status\": \"success\",\n","                                \"method\": \"fabric_utils\",\n","                                \"size_bytes\": file_size,\n","                                \"type\": file_info[\"type\"]\n","                            }\n","                            \n","                        except Exception as fabric_error:\n","                            log_message(f\"    ‚ö†Ô∏è  Fabric utils failed for {relative_path}, trying Spark method: {str(fabric_error)}\", \"WARNING\")\n","                            \n","                            # Fallback to Spark method\n","                            success = restore_file_with_spark(file_content, target_file_path, relative_path, file_info)\n","                            if success:\n","                                files_restored += 1\n","                                total_size_restored += file_size\n","                                restore_details[relative_path] = {\n","                                    \"status\": \"success\",\n","                                    \"method\": \"spark_fallback\",\n","                                    \"size_bytes\": file_size,\n","                                    \"type\": file_info[\"type\"],\n","                                    \"note\": \"Stored as Delta table due to Fabric utils limitation\"\n","                                }\n","                            else:\n","                                files_failed += 1\n","                                restore_details[relative_path] = {\n","                                    \"status\": \"failed\",\n","                                    \"error\": \"Both Fabric utils and Spark methods failed\"\n","                                }\n","                    \n","                    else:\n","                        # Method 2: Spark-only method (stores as Delta)\n","                        success = restore_file_with_spark(file_content, target_file_path, relative_path, file_info)\n","                        if success:\n","                            files_restored += 1\n","                            total_size_restored += file_size\n","                            restore_details[relative_path] = {\n","                                \"status\": \"success\",\n","                                \"method\": \"spark_only\",\n","                                \"size_bytes\": file_size,\n","                                \"type\": file_info[\"type\"],\n","                                \"note\": \"Stored as Delta table (Fabric utils not available)\"\n","                            }\n","                        else:\n","                            files_failed += 1\n","                            restore_details[relative_path] = {\n","                                \"status\": \"failed\",\n","                                \"error\": \"Spark method failed\"\n","                            }\n","                    \n","                except Exception as file_error:\n","                    log_message(f\"    ‚ùå Failed to restore file {relative_path}: {str(file_error)}\", \"ERROR\")\n","                    restore_details[relative_path] = {\"status\": \"failed\", \"error\": str(file_error)}\n","                    files_failed += 1\n","        \n","        log_message(f\"üìÅ File restore completed: {files_restored} restored, {files_skipped} skipped, {files_failed} failed\", \"INFO\")\n","        log_message(f\"üìä Total size restored: {total_size_restored/1024/1024:.2f} MB\", \"INFO\")\n","        \n","        return {\n","            \"success\": True,\n","            \"files_restored\": files_restored,\n","            \"files_skipped\": files_skipped,\n","            \"files_failed\": files_failed,\n","            \"total_size_mb\": total_size_restored / 1024 / 1024,\n","            \"restore_details\": restore_details\n","        }\n","        \n","    except Exception as e:\n","        log_message(f\"File restore failed: {str(e)}\", \"ERROR\")\n","        return {\"success\": False, \"error\": str(e)}\n","\n","def restore_file_with_spark(file_content, target_file_path, relative_path, file_info):\n","    \"\"\"Restore file using Spark (fallback method)\"\"\"\n","    try:\n","        # Create a DataFrame with the file content\n","        file_df = spark.createDataFrame([{\n","            \"original_path\": relative_path,\n","            \"file_name\": file_info[\"name\"],\n","            \"file_type\": file_info[\"type\"],\n","            \"content\": file_content,\n","            \"size_bytes\": len(file_content),\n","            \"restored_timestamp\": datetime.datetime.now().isoformat(),\n","            \"restoration_note\": \"File content preserved in binary format within Delta table\"\n","        }])\n","        \n","        # Save as Delta table\n","        file_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{target_file_path}_restored\")\n","        \n","        log_message(f\"    ‚úÖ Restored file via Spark: {relative_path} (stored as Delta)\", \"INFO\")\n","        return True\n","        \n","    except Exception as e:\n","        log_message(f\"    ‚ùå Spark file restore failed for {relative_path}: {str(e)}\", \"ERROR\")\n","        return False\n","\n","print(\"‚úÖ File restore functions defined successfully\")\n","\n","# ============================================================================\n","# CELL 8: Verification Functions\n","# ============================================================================\n","\n","def verify_restored_data(target_paths, zip_contents, table_restore_result, file_restore_result):\n","    \"\"\"Verify that restored data matches the backup\"\"\"\n","    try:\n","        if not verify_restore:\n","            log_message(\"‚è≠Ô∏è  Verification skipped (verify_restore=False)\", \"INFO\")\n","            return {\"skipped\": True}\n","        \n","        log_message(\"üîç Starting data verification...\", \"INFO\")\n","        verification_results = {\n","            \"tables\": {},\n","            \"files\": {},\n","            \"overall_success\": True\n","        }\n","        \n","        # Verify tables\n","        if restore_tables and table_restore_result.get(\"tables_restored\", 0) > 0:\n","            log_message(\"  üìä Verifying restored tables...\", \"INFO\")\n","            \n","            for table_name, restore_info in table_restore_result.get(\"restore_details\", {}).items():\n","                if restore_info.get(\"status\") == \"success\":\n","                    try:\n","                        # Check if table is readable\n","                        table_path = f\"{target_paths['tables']}/{table_name}\"\n","                        df = spark.read.format(\"delta\").load(table_path)\n","                        actual_count = df.count()\n","                        expected_count = restore_info.get(\"row_count\", 0)\n","                        \n","                        tables_match = actual_count == expected_count\n","                        verification_results[\"tables\"][table_name] = {\n","                            \"verified\": True,\n","                            \"data_matches\": tables_match,\n","                            \"actual_rows\": actual_count,\n","                            \"expected_rows\": expected_count\n","                        }\n","                        \n","                        if tables_match:\n","                            log_message(f\"    ‚úÖ Table {table_name}: {actual_count:,} rows verified\", \"INFO\")\n","                        else:\n","                            log_message(f\"    ‚ö†Ô∏è  Table {table_name}: Row count mismatch (expected {expected_count:,}, got {actual_count:,})\", \"WARNING\")\n","                            verification_results[\"overall_success\"] = False\n","                        \n","                    except Exception as table_verify_error:\n","                        log_message(f\"    ‚ùå Failed to verify table {table_name}: {str(table_verify_error)}\", \"ERROR\")\n","                        verification_results[\"tables\"][table_name] = {\n","                            \"verified\": False,\n","                            \"error\": str(table_verify_error)\n","                        }\n","                        verification_results[\"overall_success\"] = False\n","        \n","        # Verify files\n","        if restore_files and file_restore_result.get(\"files_restored\", 0) > 0:\n","            log_message(\"  üìÅ Verifying restored files...\", \"INFO\")\n","            \n","            for file_path, restore_info in file_restore_result.get(\"restore_details\", {}).items():\n","                if restore_info.get(\"status\") == \"success\":\n","                    try:\n","                        # Check if file exists and is accessible\n","                        target_file_path = f\"{target_paths['files']}/{file_path}\"\n","                        file_exists = False\n","                        \n","                        if fabric_utils_available and restore_info.get(\"method\") == \"fabric_utils\":\n","                            try:\n","                                fabric_utils.fs.head(target_file_path, 1)\n","                                file_exists = True\n","                            except:\n","                                file_exists = False\n","                        else:\n","                            # Check Delta table\n","                            try:\n","                                file_df = spark.read.format(\"delta\").load(f\"{target_file_path}_restored\")\n","                                file_df.limit(1).collect()\n","                                file_exists = True\n","                            except:\n","                                file_exists = False\n","                        \n","                        verification_results[\"files\"][file_path] = {\n","                            \"verified\": True,\n","                            \"file_exists\": file_exists,\n","                            \"restore_method\": restore_info.get(\"method\", \"unknown\")\n","                        }\n","                        \n","                        if file_exists:\n","                            log_message(f\"    ‚úÖ File {file_path}: Accessible\", \"INFO\")\n","                        else:\n","                            log_message(f\"    ‚ö†Ô∏è  File {file_path}: Not accessible after restore\", \"WARNING\")\n","                            verification_results[\"overall_success\"] = False\n","                        \n","                    except Exception as file_verify_error:\n","                        log_message(f\"    ‚ùå Failed to verify file {file_path}: {str(file_verify_error)}\", \"ERROR\")\n","                        verification_results[\"files\"][file_path] = {\n","                            \"verified\": False,\n","                            \"error\": str(file_verify_error)\n","                        }\n","                        verification_results[\"overall_success\"] = False\n","        \n","        tables_verified = len([t for t in verification_results[\"tables\"].values() if t.get(\"verified\")])\n","        files_verified = len([f for f in verification_results[\"files\"].values() if f.get(\"verified\")])\n","        \n","        log_message(f\"üîç Verification complete: {tables_verified} tables, {files_verified} files verified\", \"INFO\")\n","        \n","        if verification_results[\"overall_success\"]:\n","            log_message(\"‚úÖ All verification checks passed\", \"INFO\")\n","        else:\n","            log_message(\"‚ö†Ô∏è  Some verification checks failed - see details above\", \"WARNING\")\n","        \n","        return verification_results\n","        \n","    except Exception as e:\n","        log_message(f\"Verification failed: {str(e)}\", \"ERROR\")\n","        return {\"success\": False, \"error\": str(e)}\n","\n","print(\"‚úÖ Verification functions defined successfully\")\n","\n","# CELL 9: Fixed Main Restore Execution (replace the manifest creation part)\n","# ============================================================================\n","\n","try:\n","    start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    log_message(\"üîÑ Starting Microsoft Fabric Lakehouse COMPLETE RESTORE\", \"INFO\")\n","    log_message(\"üìã Restoring Tables and Files from unified backup\", \"INFO\")\n","    \n","    # ... (keep all the existing code until the manifest creation part)\n","    \n","    # Execute restore (keep existing code)\n","    table_restore_result = restore_tables_from_zip(zip_contents, target_paths['tables'])\n","    file_restore_result = restore_files_from_zip(zip_contents, target_paths['files'])\n","    verification_result = verify_restored_data(target_paths, zip_contents, table_restore_result, file_restore_result)\n","    \n","    # Record end time\n","    end_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    \n","    # Create restore manifest - FIXED VERSION\n","    restore_manifest_simple = {\n","        \"restore_id\": str(uuid.uuid4())[:8],\n","        \"restore_timestamp\": get_current_timestamp(),\n","        \"source_backup_path\": backup_source_path,\n","        \"target_lakehouse\": target_lakehouse_name,\n","        \"restore_type\": \"complete_lakehouse_restore\",\n","        \"fabric_utils_available\": fabric_utils_available,\n","        \"fabric_utils_name\": utils_name,\n","        \"dry_run\": dry_run,\n","        \"start_time\": start_time,\n","        \"end_time\": end_time,\n","        \"duration_seconds\": (datetime.datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\") - \n","                             datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds(),\n","        \"backup_type\": backup_info.get(\"backup_type\", \"unknown\"),\n","        \"restore_method\": backup_info.get(\"restore_method\", \"unknown\"),\n","        \"tables_restored\": table_restore_result.get(\"tables_restored\", 0),\n","        \"tables_failed\": table_restore_result.get(\"tables_failed\", 0),\n","        \"files_restored\": file_restore_result.get(\"files_restored\", 0),\n","        \"files_failed\": file_restore_result.get(\"files_failed\", 0),\n","        \"verification_passed\": verification_result.get(\"overall_success\", False)\n","    }\n","    \n","    if not dry_run:\n","        # Save restore manifest - FIXED VERSION\n","        try:\n","            manifest_df = spark.createDataFrame([restore_manifest_simple])\n","            manifest_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{target_paths['files']}/_restore_manifest_{get_current_timestamp()}\")\n","            log_message(\"üìù Restore manifest saved successfully\", \"INFO\")\n","        except Exception as manifest_error:\n","            log_message(f\"Warning: Could not save restore manifest: {str(manifest_error)}\", \"WARNING\")\n","    \n","    # Write logs\n","    if enable_detailed_logging and log_entries and not dry_run:\n","        try:\n","            log_schema = StructType([\n","                StructField(\"timestamp\", StringType(), True),\n","                StructField(\"level\", StringType(), True),\n","                StructField(\"message\", StringType(), True)\n","            ])\n","            \n","            log_rows = [(entry[\"timestamp\"], entry[\"level\"], entry[\"message\"]) for entry in log_entries]\n","            log_df = spark.createDataFrame(log_rows, log_schema)\n","            log_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{target_paths['files']}/_restore_logs_{get_current_timestamp()}\")\n","            log_message(\"üìù Restore logs written to target lakehouse\", \"INFO\")\n","        except Exception as e:\n","            log_message(f\"Error writing restore logs: {str(e)}\", \"WARNING\")\n","    \n","    # Final summary\n","    duration_seconds = (datetime.datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\") - \n","                        datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n","    \n","    log_message(\"\", \"INFO\")\n","    log_message(\"üéâ \" + \"=\" * 58, \"INFO\")\n","    log_message(\"üéâ LAKEHOUSE RESTORE COMPLETED!\", \"INFO\")\n","    log_message(\"üéâ \" + \"=\" * 58, \"INFO\")\n","    log_message(f\"‚úÖ Restore finished at: {end_time}\", \"INFO\")\n","    log_message(f\"üìÇ Target lakehouse: {target_lakehouse_name}\", \"INFO\")\n","    log_message(f\"üíæ Source backup: {backup_source_path}\", \"INFO\")\n","    \n","    if dry_run:\n","        log_message(\"üîç DRY RUN COMPLETED - No actual changes made\", \"INFO\")\n","    else:\n","        tables_restored = table_restore_result.get(\"tables_restored\", 0)\n","        files_restored = file_restore_result.get(\"files_restored\", 0)\n","        \n","        log_message(f\"üìä Tables restored: {tables_restored}\", \"INFO\")\n","        log_message(f\"üìÅ Files restored: {files_restored}\", \"INFO\")\n","        \n","        if 'total_size_mb' in file_restore_result:\n","            log_message(f\"üì¶ Data size restored: {file_restore_result['total_size_mb']:.2f} MB\", \"INFO\")\n","        \n","        if verification_result.get(\"overall_success\"):\n","            log_message(\"‚úÖ Verification: All checks passed\", \"INFO\")\n","        elif verification_result.get(\"skipped\"):\n","            log_message(\"‚è≠Ô∏è  Verification: Skipped\", \"INFO\")\n","        else:\n","            log_message(\"‚ö†Ô∏è  Verification: Some checks failed\", \"WARNING\")\n","    \n","    log_message(f\"‚è±Ô∏è  Duration: {duration_seconds:.2f} seconds\", \"INFO\")\n","    log_message(\"=\" * 60, \"INFO\")\n","    \n","    # Final result\n","    final_result = {\n","        \"status\": \"success\",\n","        \"restore_type\": \"complete_lakehouse\",\n","        \"target_lakehouse\": target_lakehouse_name,\n","        \"source_backup\": backup_source_path,\n","        \"duration_seconds\": duration_seconds,\n","        \"dry_run\": dry_run,\n","        \"tables_restored\": table_restore_result.get(\"tables_restored\", 0),\n","        \"files_restored\": file_restore_result.get(\"files_restored\", 0),\n","        \"verification_passed\": verification_result.get(\"overall_success\", False),\n","        \"backup_type\": backup_info[\"backup_type\"],\n","        \"restore_method\": backup_info[\"restore_method\"]\n","    }\n","    \n","    print(f\"\\nüéâ LAKEHOUSE RESTORE RESULT:\")\n","    print(json.dumps(final_result, indent=2, default=str))\n","    \n","except Exception as e:\n","    log_message(f\"üí• Lakehouse restore FAILED: {str(e)}\", \"ERROR\")\n","    import traceback\n","    log_message(f\"Full error trace: {traceback.format_exc()}\", \"ERROR\")\n","    \n","    failure_result = {\n","        \"status\": \"failed\",\n","        \"error\": str(e),\n","        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    }\n","    \n","    print(\"\\nüí• LAKEHOUSE RESTORE RESULT:\")\n","    print(json.dumps(failure_result, indent=2))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"cdb7285e-ee2d-4802-b96f-d1eac5fce30b","normalized_state":"finished","queued_time":"2025-07-03T20:53:05.2266807Z","session_start_time":null,"execution_start_time":"2025-07-03T20:53:05.2326304Z","execution_finish_time":"2025-07-03T20:53:40.5044088Z","parent_msg_id":"99ab600d-e069-4e86-917a-39538dd5e46a"},"text/plain":"StatementMeta(, cdb7285e-ee2d-4802-b96f-d1eac5fce30b, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üìö Libraries imported successfully\nüîß Fabric utilities available: True (mssparkutils)\nüîÑ Lakehouse restore process initiated at: 2025-07-03 20:53:05.483703\n‚úÖ Core helper functions defined successfully\n‚úÖ Path construction functions defined successfully\n‚úÖ Backup analysis and ZIP reading functions defined successfully\n‚úÖ FIXED Table restore functions defined successfully\n‚úÖ File restore functions defined successfully\n‚úÖ Verification functions defined successfully\n[2025-07-03 20:53:05] [INFO] üîÑ Starting Microsoft Fabric Lakehouse COMPLETE RESTORE\n[2025-07-03 20:53:05] [INFO] üìã Restoring Tables and Files from unified backup\n[2025-07-03 20:53:05] [INFO] üìä Starting table restore for 2 tables...\n[2025-07-03 20:53:05] [INFO]   üìã Restoring table: customers\n[2025-07-03 20:53:22] [WARNING]     ‚ö†Ô∏è  Table customers exists, skipping (overwrite_existing=False)\n[2025-07-03 20:53:22] [INFO]   üìã Restoring table: products\n[2025-07-03 20:53:25] [WARNING]     ‚ö†Ô∏è  Table products exists, skipping (overwrite_existing=False)\n[2025-07-03 20:53:25] [INFO] üìä Table restore completed: 0 restored, 0 failed\n[2025-07-03 20:53:25] [INFO] üìÅ Starting file restore for 1 files...\n[2025-07-03 20:53:25] [INFO]   üìÑ Restoring file: car_manufacturing_data_pipeline.svg\n[2025-07-03 20:53:25] [WARNING]     ‚ö†Ô∏è  File car_manufacturing_data_pipeline.svg exists, skipping (overwrite_existing=False)\n[2025-07-03 20:53:25] [INFO] üìÅ File restore completed: 0 restored, 1 skipped, 0 failed\n[2025-07-03 20:53:25] [INFO] üìä Total size restored: 0.00 MB\n[2025-07-03 20:53:25] [INFO] üîç Starting data verification...\n[2025-07-03 20:53:25] [INFO] üîç Verification complete: 0 tables, 0 files verified\n[2025-07-03 20:53:25] [INFO] ‚úÖ All verification checks passed\n[2025-07-03 20:53:37] [INFO] üìù Restore manifest saved successfully\n[2025-07-03 20:53:39] [INFO] üìù Restore logs written to target lakehouse\n[2025-07-03 20:53:39] [INFO] \n[2025-07-03 20:53:39] [INFO] üéâ ==========================================================\n[2025-07-03 20:53:39] [INFO] üéâ LAKEHOUSE RESTORE COMPLETED!\n[2025-07-03 20:53:39] [INFO] üéâ ==========================================================\n[2025-07-03 20:53:39] [INFO] ‚úÖ Restore finished at: 2025-07-03 20:53:25\n[2025-07-03 20:53:39] [INFO] üìÇ Target lakehouse: lh_msft_bae_restore\n[2025-07-03 20:53:39] [INFO] üíæ Source backup: abfss://WS-MSFT-BAE-DEMO-SBX@onelake.dfs.fabric.microsoft.com/lh_msft_bae_backup.Lakehouse/Files/complete_backup_2025-07-03_20-41-04_64af2286\n[2025-07-03 20:53:39] [INFO] üìä Tables restored: 0\n[2025-07-03 20:53:39] [INFO] üìÅ Files restored: 0\n[2025-07-03 20:53:39] [INFO] üì¶ Data size restored: 0.00 MB\n[2025-07-03 20:53:39] [INFO] ‚úÖ Verification: All checks passed\n[2025-07-03 20:53:39] [INFO] ‚è±Ô∏è  Duration: 20.00 seconds\n[2025-07-03 20:53:39] [INFO] ============================================================\n\nüéâ LAKEHOUSE RESTORE RESULT:\n{\n  \"status\": \"success\",\n  \"restore_type\": \"complete_lakehouse\",\n  \"target_lakehouse\": \"lh_msft_bae_restore\",\n  \"source_backup\": \"abfss://WS-MSFT-BAE-DEMO-SBX@onelake.dfs.fabric.microsoft.com/lh_msft_bae_backup.Lakehouse/Files/complete_backup_2025-07-03_20-41-04_64af2286\",\n  \"duration_seconds\": 20.0,\n  \"dry_run\": false,\n  \"tables_restored\": 0,\n  \"files_restored\": 0,\n  \"verification_passed\": true,\n  \"backup_type\": \"zip_file\",\n  \"restore_method\": \"zip_direct\"\n}\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86bac095-efd7-4563-8682-1772c558b430"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"kernel_info":{"name":"synapse_pyspark"}},"nbformat":4,"nbformat_minor":5}